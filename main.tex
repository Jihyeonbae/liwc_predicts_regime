%TC:ignore
\immediate\write18{texcount -inc -sum -utf8 main.tex > wordcount.txt}
%TC:endignore



\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{newtxtext,newtxmath} % Better than \usepackage{times}
\usepackage{hyperref}
\usepackage{setspace}
\doublespacing
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{siunitx}
\usepackage{tabularray}
\usepackage{indentfirst}

\sisetup{detect-all} % ensures consistency with font settings
\usepackage[style=apa, backend=biber]{biblatex}
\addbibresource{LIWC.bib}



\title{How do the Leaders Speak? Prediction of Regime Type Using Linguistic Features}
\author{Jihyeon Bae, Valentina Staneva}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Studies of international rhetoric typically focus on what leaders say: the concepts, frames, and keywords that appear in their speeches. This note argues that how leaders speak is equally important for understanding regime differences in global fora. Using a corpus of United Nations General Debate speeches, we apply random forest models to a rich set of linguistic indicators from the LIWC dictionary. The results show that stylistic and structural features, such as average sentence length, grammatical composition, and punctuation use, emerge as consistently important predictors of whether a speaker represents a democracy or an autocracy. These variables often rival or surpass the predictive power of more substantive content features. Importantly, random forest accuracy does not substantially exceed a logistic regression baseline, suggesting that the main contribution of machine learning here is not greater prediction but the identification of stylistic signals embedded in political speech. The analysis highlights the need to treat rhetorical style as more than noise. It provides a systematic dimension through which regime type is communicated and distinguished. By foregrounding how leaders speak alongside what they say, this note calls for greater attention to stylistic dimensions of international discourse in both empirical research and theoretical debates.
\end{abstract}

\keywords{regime type;  political rhetoric;   machine learning; text analysis}

\section{Introduction}

Studies of international rhetoric typically focus on what leaders say: the topics, principles, and keywords invoked in their speeches. This paper highlights that how leaders speak also provides systematic information about regime type. We use linguistic features of state representatives’ speeches from the United Nations General Debate Corpus (UNGDC) to predict whether a country is democratic or authoritarian. 

The contribution is twofold. First, we provide a hard test of the claim that regime types differ in their rhetorical style. If linguistic features alone can predict the regime type of a speaker, this offers strong evidence that stylistic choices—such as sentence length, grammatical composition, or the balance of function words—systematically vary across political systems. Second, we identify the specific features that act as the strongest signals of regime type and assess their robustness across modeling strategies. 

To do so, we apply the Linguistic Inquiry and Word Count (LIWC) dictionary to all UN General Debate speeches and merge the results with country–year regime classifications. We then compare random forest models to a logistic regression baseline. The findings previewed here suggest that while predictive performance is modest, certain stylistic features consistently emerge as important, underscoring the value of analyzing rhetorical style alongside substantive content in the study of international discourse.

This script uses the scores of LIWC features and merges with country-year level meta data. Our dataset has a battery of country-year level variables that might potentially confound the statistical modeling. At the country-year level, we run a series of statistical models that probe the relationship between linguistic features and sentiment scores of that speech. To preview, LIWC features alone have a strong predictive power on regime types, even without the help of meta data. 

We test whether there is a correlation between a country's invocation of international legal norms and the regime type. Among many, we generate three key legal principles that are prominent throughout the history of international politics. These are principle of sovereignty, principle of non-intervention, and the principle of human rights. Binary variables capture whether each principle was invoked, and count variables measure the number of time it was mentioned within one speech.  

\section{Autocratic and Democratic Language Features}

\subsection{Linguistic Style as a Predictor}


\section{Data and Methods}

I included linguistic style features as well as word counts(WC), the raw number of words within each speech Word counts can change the distribution of linguistic features, as the pool from which words come from can affect the count of dictionary words. Another alternative measures is Words per Sentence (WPS), average number of words within a sentence for each document. Big Words (BW) also captures the percentage of words that are 7 letters or longer. Dictionary (Dic) variable refers to the percentage of words that were captured by LIWC.

There are four summary variables; Analytical, Clout, Authenticity, and Emotional Tone. Each are constructed based on LIWC features as well as other psychological literature \parencite{boyd_did_2015, cohn_linguistic_2004, kacewicz_pronoun_2014, newman_lying_2003}. These summary variables range from 1 to 99, after standardization. Analytical thinking score increases when the speech includes formal, logical, and hierarchical patterns. Clout is intended to capture the degree in which the speaker displays one's confidence, status, and leadership. Authenticity score increases when the speaker does not intend to regulate oneself and displays a spontaneous style. Emotional score is an overall tone of the speech, intended to show how positive or negative a speaker is.

Note that one word can be captured by different bags of dictionaries. For example, the word ``we`` will be counted toward a ``affiliation``, ``social reference``, and a pronoun dictionary.

LIWC-22 has additional features like ``determiners,`` that includes ``this``, ``first``, ``three.`` It also has a feature ``Cognition,`` that reflects different ways in which people process their thoughts. It is shown by dichotomous logical words, memory words, and words that reveal certainty. It also includes ``political`` as one of the three domains within the overarching ``Culture`` dimension. Examples for ``political`` construct are congress, parliament, president, democratic, law, and court.

\subsection{Features}
The variable ``Dic(Dictionary Words)`` show the percentage of words in the text that are captured by one or more LIWC feature dictionaries.``WC(Word Count)`` refers to the total word counts per document, and ``WPS(Word Per Sentences)`` shows the average number of words for sentences per document. ``WPS`` is negatively correlated with ``Period,`` in that a speech with long sentences need less periods to end each sentence.

\subsection{Random Forest Model}
To compare the performance of logistic regression, we use non-parametric random forest models to see how well a battery of LIWC features can predict the regime type.
Random Forest approach bootstraps samples multiple times with replacement. For each iteration, a decision tree is built and the algorithm trains the model through all the trees. The model uses the majority vote from all trees to reach a conclusion on the given classification task. It is important to explicitly set the output variable, democracy, as a factor variable, so that R recognizes the task as a classification, not a regression model.

The Mean Decrease Accuracy (MDA) plot expresses how much accuracy the model loses by permuting each variable. The more the accuracy suffers, the more important the variable is for the successful classification. According to the \textcite{liaw_classification_2002} R package, the measure is computed from permuting out of bag data. Each tree generates prediction error for out-of-bag data and another prediction error after permuting one variable. Each tree records difference in two errors with and without permutation across all the features, and eventually generates an average. MDA is then divided by the standard deviation value of decrease in accuracy of trees. 

The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model. Since we're interested in the substantive meaning of feature, we focus on the accuracy score instead of Gini coefficient.

\input{}
\section{Statistical Models}
To supplement machine-learning approach of regression, I present a set of conventional statistical tests accounting for error correlation.  Using random effects model when the unit-specific intercepts have correlation with the input variables, and eventually lead to omitted variable bias. I suspect this would not be the case, as the linguistic features (inputs) often have high correlation with time invariant characteristics of countries. However, there does not exist a consistent theoretical conjecture on the correlation between these two. To accommodate such uncertainty, we also supplement the result by using fixed effects model.

\section{Empirical Results and Model Performance}
As a baseline, the first trial keeps all of the LIWC features to the random forest model. This hit the accuracy rate of 0.7496. To make sure there is enough observations for both democracy and non-democracy, we use stratified sampling based on the main output of interest, \textit{democracy}. This means that the model performance can change when the distribution of the output is different from the current dataset. Our dataset has a well balanced distribution, with 4883 non-democracies and 4763 democracies. We randomly keep 30\% of the data as a testing dataset. Functionally, we use R packages \textcite{paluszynska_randomforestexplainer_2020} and \textcite{liaw_classification_2002} to generate various metrics of Importance. 

Based on several importance metrics, ``WPS`` showed up as important features. Given theoretically weak ties between WPS and regime type, we investigate sources of biases in the model. The specific question here is \textit{to what extent is variable ``WPS`` robust against different modeling strategies and importance metrics selections}?

\section{Conclusion}


\begin{appendices}


\end{appendices}

\printbibliography

\end{document}
