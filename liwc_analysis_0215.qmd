---
title: "liwc_modeling"
author: "Jihyeon bae"
date: "`r Sys.Date()`"
output: html_document
bibliography: ../references.bib
---

# Introduction

In this project, I use linguistic features of state representatives' speech transcripts from the United Nations General Debate Corpus (UNGDC) to predict the regime type. The goal is twofold. First, we aim at running a hard test for a hypothesis that countries identified with distinct regime types show different linguistic styles. If I can predict the speaker's regime type based on linguistic features, it is a strong indication of the difference in linguistic features across regime types. Second, this project analyzes key linguistic features that act as a strong signal of the state's regime type. I further interpret substantive implication of strong coefficients and check how consistent their degrees of significance are across the models.

This script uses the scores of LIWC features and merges with country-year level meta data. `"data/raw/controls/controls.csv"` has a battery of country-year level variables that might potentially confound the statistical modeling. With `liwc_meta` dataset, at the country-year level, I run a series of statistical models that probe the relationship between linguistic features and sentiment scores of that speech. To preview, LIWC features alone have a strong predictive power on regime types, even without the help of meta data. 

I test whether there is a correlation between a country's invocation of international legal norms and the regime type. Among many, I generate three key legal principles that are prominent throughout the history of international politics. These are principle of sovereignty, principle of non-intervention, and the principle of human rights. Binary variables capture whether each principle was invoked, and count variables measure the number of time it was mentioned within one speech.  

```{r setup, include=FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(countrycode)
library(lmtest) #for coeftest function
library(collapse)
library(plm)
library(gplm)
library(broom)
library(knitr) # for kable function
library(readxl)
library(pROC)
library(visreg)
library(rsample)
library(groupdata2)
library(cvms)
library(xpectr)
library(kableExtra)
library(modelsummary)


liwc_df<-read_csv("data/interim/liwc_meta.csv")

liwc_df<-liwc_df%>%
  dplyr::select(-"...1")
liwc_df$year<-as.numeric(liwc_df$year)

#removing variables with too many missing values (missingness above 70%)
liwc_df <- liwc_df %>%
  dplyr::select(
    -cow_num_inter, -cow_inter, -cow_num_civil, -cow_civil, 
    -wdi_homicides, -wjp_overall, -wbgi_rle, -wbgi_pve, 
    -wbgi_cce, -wbgi_gee, -pts_ptsh, -rsf_pfi, -rsf_pfi0212, 
    -rsf_pfi1321, -nelda_mbbe, -nelda_noe, -nelda_oa, 
    -nelda_rpae, -nelda_vcdbe, -mm_num_violence, -dpi_execnat, 
    -dpi_legelec, -dpi_exelec, -dpi_liec, -dpi_eiec, -dpi_execrlc,
    -dpi_fraud, -dpi_auton, -dpi_muni, 
    -v2eltype_0, -v2eltype_1, -v2eltype_2, -v2eltype_3, 
    -v2eltype_4, -v2eltype_5, -v2eltype_6, -pts_ptsa, -mm_num_protest, -v2mecenefi, -v2mecenefibin, -v2smgovdom,
    -v2smgovfilcap, -v2smgovfilprc, -v2smgovshut, -v2smgovshutcap
  )
```

# Variables on legal norms

-   Three binary variables: `sovereignty`, `intervention`, and `human_rights`. These are coded 1 when detected at least one time during the speech, 0 otherwise.
-   Three count variables: `sovereignty_count`, `intervention_count`, `human_rights_count`. These variables are the number of keywords appearing during the speech.

```{r}
liwc_df <- liwc_df %>%
  mutate(sovereignty = ifelse(str_detect(text, "sovereignty"), 1, 0),
         intervention = ifelse(str_detect(text, "intervention"), 1, 0),
         human_rights = ifelse(str_detect(text, "human rights"), 1, 0)) %>%
  mutate(sovereignty_count = str_count(text, "sovereignty"),
         intervention_count = str_count(text, "intervention"),
         human_rights_count = str_count(text, "human rights"))
```

# Statistical Modeling

Using random effects model when the unit-specific intercepts have correlation with the input variables, and eventually lead to omitted variable bias. I suspect this would not be the case, as the linguistic features (inputs) often have high correlation with time invariant characteristics of countries. However, there does not exist a consistent theoretical conjecture on the correlation between these two. To accommodate such uncertainty, we also supplement the result by using fixed effects model.

### Model 0: pooled model with LIWC features

plm package does not allow a dot feature ( y ~ . ) which selects all the columns except for the specified dependent variable. In order to avoid manual entry of all the LIWC features, I use a function called "expand_formula" from a [StackOverflow](https://stackoverflow.com/questions/26182348/use-all-variables-in-a-model-with-plm-in-r).

```{r}
  "
  Input: dependent variable in a character form, names of the input features
  Output: formula ready to be used for plm package. 
  Example: expand_formula(output ~ .,names(data)) 
            output ~ x1 + x2 + ... + xn
  "
expand_formula <- 
  function(form="A ~.",varNames=c("A","B","C")){
  has_dot <- any(grepl('.',form,fixed=TRUE))
  if(has_dot){
    ii <- intersect(as.character(as.formula(form)),
          varNames)
    varNames <- varNames[!grepl(paste0(ii,collapse='|'),varNames)]

   exp <- paste0(varNames,collapse='+')
   as.formula(gsub('.',exp,form,fixed=TRUE))

  }
  else as.formula(form)
}



```

- I create separate blocks of variables to adjust inclusion of metadqta for later analyses. 
```{r}
liwc_df<-liwc_df%>%
  rename("function_feature" = "function")

liwc_df <-  liwc_df[liwc_df$year < 2021, ]
validation <- liwc_df[liwc_df$year >= 2021, ]


liwc_inputs<-liwc_df[, 5:122]
id <- liwc_df[, 1:3]
y <- liwc_df[, 149]
controls <- liwc_df[, c(126:148, 150:191)]
data <- cbind(y, id, liwc_inputs)
data_controls<- cbind(y, id, liwc_inputs, controls)
controls_only<- cbind(y, id, controls)


```

# Result of statistical analysis
To supplement machine-learning approach of regression, I present a set of conventional statistical tests accounting for error correlation.  
```{r}
data<-pdata.frame(data, index=c("ccode_iso","session", "year"), drop.index = TRUE)

pdim(data)

model <-plm(expand_formula("dd_democracy ~. ", names(data)),
             data = data,
             index = c("ccode_iso", "year"),
             model = "within"
 )

# standard error clustered by both time and group.
model_cluster_twoway <- kable(
  tidy(coeftest(model, vcov=vcovDC(model, type="sss",
  caption= "Pooled model with cluster robust standard errors"))))

# standard error clustered by time. 
model_cluster_time<- kable(
  tidy(coeftest(model, vcov=vcovHC(model, type="sss", cluster="time"),
  caption = "Pooled model with clustering around time")))
  
# standard error clustered by country 
model_cluster_country <- kable(
  tidy(coeftest(model, vcov=vcovHC(model, type="sss", cluster="group"),
  caption = "Pooled model with clustering around country")))

model_summary <- modelsummary(
  model, 
  stars = TRUE,
  vcov = list(
    vcovDC(model, type = "sss"),
    vcovHC(model, type = "sss", cluster = "time"),
    vcovHC(model, type = "sss", cluster = "group")
  ),
  caption = "Pooled model with different types of standard errors",
  output = "kableExtra",
  escape = FALSE
) 


model_summary %>%kable_classic("hover", 
                               full_width=F,
                               html_font = "Cambria")%>%
    scroll_box(width = "100%", height = "400px")

```

# Prediction regression and cross valication

## Separate test data from validation set.

Take out the test data set (just a few years) and then split for th e validation data. In my dataset, I carve out observations from two years (2021 and 2022) as my test data.

Within the test data, I split the data in to two groups: pre and post Cold War with a threshold of 1990. I create several models based on the pre-Cold War era and generate model evaluation metrics by applying the models to the post Cold War era.

# Experiment 1: Random CV

This includes both country level meta data and LIWC features. I split the training and testing dataset randomly.

```{r}
data_controls$ccode_iso<-as.factor(data_controls$ccode_iso)
# Remove the country variable from the training data
data_controls <- data_controls[, !(names(data_controls) %in% c("ccode_iso", "year", "session", "dd_regime"))]
data_controls <- data_controls[, !(names(data_controls) %in% c("democracy"))]
data_controls<-data_controls%>%dplyr::select(-Segment)

set.seed(1)
data_controls$dd_democracy<-as.factor(data_controls$dd_democracy)

split <- rsample::initial_split(data_controls, prop = 0.7)
trainN <- rsample::training(split)
testN <- rsample::testing(split)

model0 <- glm(
  formula = dd_democracy ~ . ,
  family = "binomial",
  data = trainN)


model0_pred<-predict(model0, testN, type = "response")

model0_diagnosis <- as.factor(ifelse(model0_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model0_diagnosis), as.factor(testN$dd_democracy))


```

## Interpretation of the model

Summary table of the estimation results highlight features that play important role in predicting the regime type. Below plot displays the fitted model of how a linguistic feature of "focuspast" affects an outcome of regime type. It seems that there is a weak but consistent positive correlation between the linguistic tendency to focus on the past and the regime type. This pattern is consistent regardless of a country's history of being a former colony.

```{r, warning=FALSE}

model_summary2<-modelsummary(model0, 
                             stars = TRUE, 
                             output = "kableExtra", 
                             escape = FALSE)

model_summary2%>%kable_classic(full_width=F, html_font = "Cambria")%>%
    scroll_box(width = "100%", height = "600px")
   

visreg(model0, xvar = "focusfuture", by = "ht_colonial", scale = "linear")
```

# Experiment 2: LIWC features only

I included linguistic style features as well as word counts(WC), the raw number of words within each speech Word counts can change the distribution of linguistic features, as the pool from which words come from can affect the count of dictionary words. Another alternative measures is Words per Sentence (WPS), average number of words within a sentence for each document. Big Words (BW) also captures the percentage of words that are 7 letters or longer. Dictionary (Dic) variable refers to the percentage of words that were captured by LIWC.

There are four summary variables; Analytical, Clout, Authenticity, and Emotional Tone. Each are constructed based on LIWC features as well as other psychological literature [@boyd2015; @cohn2004; @kacewicz2014; @newman2003]. These summary variables range from 1 to 99, after standardization. Analytical thinking score increases when the speech includes formal, logical, and hierarchical patterns. Clout is intended to capture the degree in which the speaker displays one's confidence, status, and leadership. Authenticity score increases when the speaker does not intend to regulate oneself and displays a spontaneous style. Emotional score is an overall tone of the speech, intended to show how positive or negative a speaker is.

Note that one word can be captured by different bags of dictionaries. For example, the word "we" will be counted toward a "affiliation", "social reference", and a pronoun dictionary.

LIWC-22 has additional features like "determiners," that includes "this", "first", "three." It also has a feature "Cognition," that reflects different ways in which people process their thoughts. It is shown by dichotomous logical words, memory words, and words that reveal certainty. It also includes "political" as one of the three domains within the overarching "Culture" dimension. Examples for "political" construct are congress, parliament, president, democratic, law, and court.

```{r}
data <- cbind(y, id, liwc_inputs)
data <- data[, !(names(data) %in% c("ccode_iso", "year"))]

split <- rsample::initial_split(data, prop = 0.7, strata = "dd_democracy")
trainN <- rsample::training(split)
testN <- rsample::testing(split)

model1 <- glm(
  formula = dd_democracy ~ . - Segment,
  family = "binomial",
  data = trainN)

model1_pred<-predict(model1, testN, type = "response")

model1_diagnosis <- as.factor(ifelse(model1_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model1_diagnosis), as.factor(testN$dd_democracy))

# sensitivity is the true positive rate
# specificity is the true negative rate

ROC<-roc(response = testN$dd_democracy, 
    predictor = model1_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")
```

# Experiment 2b: LIWC features only with a smaller dataset

When I trained the model with only a small number of training dataset and tested against a large number test dataset, the model perform poorly with 52% of accuracy, as expected.

```{r}

small_trainN <- data %>%
  sample_n(size = 100, replace = FALSE) 

model1b <- glm(
  formula = dd_democracy ~ . ,
  family = "binomial",
  data = small_trainN)

model1b_pred<-predict(model1b, testN, type = "response")

model1b_diagnosis <- as.factor(ifelse(model1b_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model1b_diagnosis), as.factor(testN$dd_democracy))

ROC<-roc(response = testN$dd_democracy, 
    predictor = model1b_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")
```

# Experiment 3: Country level meta data only

I train the model using country-year level meta data alone. Caveat here is that some of the input features from the V-Dem data set are highly correlated with the output feature, "dd_democracy." However, this model shows a high level of accuracy score of 91%.

```{r}
# Remove the country variable from the training data
controls_only <- controls_only[, !(names(controls_only) %in% c("ccode_iso", "year"))]

set.seed(2)
split <- rsample::initial_split(controls_only, prop = 0.7, strata = "dd_democracy")
trainN <- rsample::training(split)
testN <- rsample::testing(split)

model3 <- glm(
  formula = dd_democracy ~ .,
  family = "binomial",
  data = trainN)

model3_pred<-predict(model3, testN, type = "response")

model3_diagnosis <- as.factor(ifelse(model3_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model3_diagnosis), as.factor(testN$dd_democracy))

# sensitivity is the true positive rate
# specificity is the true negative rate

ROC<-roc(response = testN$dd_democracy, 
    predictor = model3_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")
```

# Experiment 4: training on Pre-Cold war, testing on post-cold war

This is a harder test. I train the model using LIWC features alone and test its performance in post-cold war era.

Accuracy : 0.6989\
No Information Rate : 0.5867\
P-Value \[Acc \> NIR\] : \< 2.2e-16\
Sensitivity : 0.5752\
Specificity : 0.7860

Change in the number of democracies after the Cold War. Predicting less democracy because it hasn't seen that many democracies.

```{r}
data <- cbind(y, id, liwc_inputs)
pre <- data[data$year < 1990, ] #dimension 4430 X 121
post <- data[data$year >= 1990, ] # dimension 6138 X 121

# Remove the country variable from the training data
pre <- pre[, !(names(pre) %in% c("ccode_iso", "year", "democracy"))]
post <- post[, !(names(post) %in% c("ccode_iso", "year", "democracy"))]

model4 <- glm(
  formula = dd_democracy ~ .,
  family = "binomial",
  data = pre)

model4_pred<-predict(model4, post, type = "response")

model4_diagnosis <- as.factor(ifelse(model4_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model4_diagnosis), as.factor(post$dd_democracy))

# sensitivity is the true positive rate
# specificity is the true negative rate

ROC<-roc(response = post$dd_democracy, 
    predictor = model4_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")

```

# Experiment 3b: training on Post-Cold war, testing on Pre-cold war

This is a harder test. I train the model using LIWC features alone and test its performance in pre-cold war era. This is a reversed version of Experiment 4. Instead of predicting the later outcome based on earlier observations, I train the data on the post-Cold war era and see if it can correctly predict the past.

Accuracy : 0.599\
95% CI : (0.5837, 0.6142) No Information Rate : 0.6357\
P-Value \[Acc \> NIR\] : 1\
Sensitivity : 0.4602\
Specificity : 0.8413

```{r}
model4b <- glm(
  formula = dd_democracy ~ .,
  family = "binomial",
  data = post)

model4b_pred<-predict(model4b, pre, type = "response")

model4b_diagnosis <- as.factor(ifelse(model4b_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model4b_diagnosis), as.factor(pre$dd_democracy))

# sensitivity is the true positive rate
# specificity is the true negative rate

ROC<-roc(response = pre$dd_democracy, 
    predictor = model4b_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")

```

# Experiment 5: Splitting based on English-speaking countries

separate out countries by removing a set of countries, english speaking countries vs. non-english speaking countries. (translator as a confounder)

-   First, load in the meta data about which language the leader chose. "Speeches are typically delivered in the native language. Based on the rules of the Assembly, all statements are then translated by UN staff into the six official languages of the UN. If a speech was delivered in a language other than English, Baturo et al.(2017) used the official English version provided by the UN. Therefore, all of the speeches in the UNGDC are in English." 



```{r}

language <-read_excel("~/Desktop/UNGDC/data/raw/language.xlsx")
language <- language%>%select(year="Year", ccode_iso = "ISO Code",lang = "Language" )

language <- language %>% 
  mutate(eng = ifelse(
    is.na(lang), NA, ifelse(
      lang %in% c("English"), 1, 0))) %>%
  select(-lang)%>%
  filter(!is.na(eng))

#baseline data includes liwc_inputs only
data <- cbind(y, id, liwc_inputs)

data<-data%>%
  inner_join(language, by=c("ccode_iso", "year"))
```

# Train dataset based on non-english and test against english
- Among 7270 speeches from 1970 until 2014, 99 were in English, while the other 3628 were in non-english. 3543 were NA values. Given a small number of samples that have complete observations of the language information after splitting into testing and training dataset, model5 performs poorly compared to the previous tests.
```{r}
eng <- data[data$eng==1, ] 
non_eng <- data[data$eng==0, ] 

# Remove the country variable from the training data
non_eng <- non_eng[, !(names(non_eng) %in% c("ccode_iso", "year", "democracy"))]
eng <- eng[, !(names(eng) %in% c("ccode_iso", "year", "democracy"))]

model5 <- glm(
  formula = dd_democracy ~ .,
  family = "binomial",
  data = non_eng)

model5_pred<-predict(model5, eng, type = "response")

model5_diagnosis <- as.factor(ifelse(model5_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model5_diagnosis), as.factor(eng$dd_democracy))

# sensitivity is the true positive rate
# specificity is the true negative rate

ROC<-roc(response = eng$dd_democracy, 
    predictor = model5_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")


```

# Experiment 5b: Train dataset based on english and test on non-english

Model 5b shows even worse performance than model5, due to its small number of sample from which the model is trained. 

```{r}

model5b <- glm(
  formula = dd_democracy ~ .,
  family = "binomial",
  data = eng)

model5b_pred<-predict(model5b, non_eng, type = "response")

model5b_diagnosis <- as.factor(ifelse(model5b_pred > 0.5, 1,0))

# Compute confusion matrix
caret::confusionMatrix(as.factor(model5b_diagnosis), as.factor(non_eng$dd_democracy))

# sensitivity is the true positive rate
# specificity is the true negative rate

ROC<-roc(response = non_eng$dd_democracy, 
    predictor = model5b_pred,
    levels = c(1, 0))

plot(ROC, col = "blue", main = "ROC Curve for Democracy Classifier")
aucValue <- auc(ROC)
print(paste("AUC:", aucValue))
text(x = 0.6, y = 0.3, label = paste("AUC =", round(aucValue, 3)), 
     cex = 1.2, col = "blue")

#save.image(file='exp5.RData')
```

# Experiment 6: Training across different ccontinents.

I split the data along geographical groups. There are two options: region and continent. Continents and Regions are defined by the World Bank Development Indicators. Experiment 6 is based on continent categories. These are Africa, Americas, Asia, Europe, and Oceania.

I use leave-one-out validation method to test one remaining continent group, based on the other four continents. The tradeoff between specificity and sensitivity scores is contingent on the proportion of democracy and non-democracy observed in the testing dataset. For example, sensitivity score is 0.44 while specificity is 0.98 when testing against Europe, after training the model with the other four continents. The proportion of democracy relative to non-democracy is higher in Europe than the other continents, leading to high number of false negatives. The same pattern of higher specificity than sensitivity is apparent when testing on American continent. Testing dataset on Africa had a balance between specificity and sensitivity, while testing result on Asia showed higher sensistivity than specificity. 

```{r}
model_summaries <- list()

data_controls<- cbind(y, id, liwc_inputs, controls)

data_controls <- data_controls %>%
  mutate(region = countrycode(ccode_iso, origin = "iso3c", destination = "region"))%>%
  mutate(continent = countrycode(ccode_iso, origin = "iso3c", destination = "continent"))

data_controls$continent<-as.factor(data_controls$continent)
data_controls <- data_controls[!is.na(data_controls$continent), ]


# Split data into 5 continent categories
continent_data <- list()

# Iterate through each continent
for (cont in unique(data_controls$continent)) {
  continent_subset <- data_controls[data_controls$continent == cont, ]
  continent_subset <- continent_subset[!is.na(continent_subset$dd_democracy), ]
  
  continent_data[[cont]] <- continent_subset
  
  positive_cases <- sum(continent_subset$dd_democracy == 1)
  negative_cases <- sum(continent_subset$dd_democracy == 0)
  cat("Continent:", cont, "\n")
  cat("Positive Cases:", positive_cases, "\n")
  cat("Negative Cases:", negative_cases, "\n\n")
}


# Iterate through each continent
for (cont in unique(data_controls$continent)) {
  # Create training and testing data based on the current continent
  train_data <- rbind(data_controls[data_controls$continent != cont, ])
  test_data <- data_controls[data_controls$continent == cont, ]
  train_data <- train_data[, !(names(train_data) %in% c("ccode_iso", "continent", "region"))]
  test_data <- test_data[, !(names(test_data) %in% c("ccode_iso", "continent", "region"))]
  
  # Train model
  model <- glm(
    formula = dd_democracy ~ .,
    family = "binomial",
    data = train_data
  )
  
  # Make predictions on test data
  model_pred <- predict(model, newdata = test_data, type = "response")
  
  # Compute evaluation metrics
  confusion_matrix <- caret::confusionMatrix(
    as.factor(ifelse(model_pred > 0.5, 1, 0)),
    as.factor(test_data$dd_democracy)
  )
  cat("Continent:", cont, "\n")
  
  # Print confusion matrix
  print(confusion_matrix)
  
  
 # Save model summary
  model_summary <- summary(model)
  
  # Add continent information to the model summary
  model_summary$continent <- cont
  
  # Append the model summary to the list
  model_summaries[[cont]] <- model_summary

}
```

# Experiment 7: Training across different ccontinents with LIWC features alone.

I rerun the same experiment excluding country level metadata to check the performance of LIWC features as standalone predictors. 
```{r}
model_summaries <- list()

data_controls<- cbind(y, id, liwc_inputs)

data_controls <- data_controls %>%
  mutate(region = countrycode(ccode_iso, origin = "iso3c", destination = "region"))%>%
  mutate(continent = countrycode(ccode_iso, origin = "iso3c", destination = "continent"))

data_controls$continent<-as.factor(data_controls$continent)
data_controls <- data_controls[!is.na(data_controls$continent), ]


# Split data into 5 continent categories
continent_data <- list()

# Iterate through each continent
for (cont in unique(data_controls$continent)) {
  continent_subset <- data_controls[data_controls$continent == cont, ]
  continent_subset <- continent_subset[!is.na(continent_subset$dd_democracy), ]
  
  continent_data[[cont]] <- continent_subset
  
  positive_cases <- sum(continent_subset$dd_democracy == 1)
  negative_cases <- sum(continent_subset$dd_democracy == 0)
  cat("Continent:", cont, "\n")
  cat("Positive Cases:", positive_cases, "\n")
  cat("Negative Cases:", negative_cases, "\n\n")
}


# Iterate through each continent
for (cont in unique(data_controls$continent)) {
  # Create training and testing data based on the current continent
  train_data <- rbind(data_controls[data_controls$continent != cont, ])
  test_data <- data_controls[data_controls$continent == cont, ]
  train_data <- train_data[, !(names(train_data) %in% c("ccode_iso", "continent", "region"))]
  test_data <- test_data[, !(names(test_data) %in% c("ccode_iso", "continent", "region"))]
  
  # Train model
  model <- glm(
    formula = dd_democracy ~ .,
    family = "binomial",
    data = train_data
  )
  
  # Make predictions on test data
  model_pred <- predict(model, newdata = test_data, type = "response")
  
  # Compute evaluation metrics
  confusion_matrix <- caret::confusionMatrix(
    as.factor(ifelse(model_pred > 0.5, 1, 0)),
    as.factor(test_data$dd_democracy)
  )
  cat("Continent:", cont, "\n")
  
  # Print confusion matrix
  print(confusion_matrix)
  
  
 # Save model summary
  model_summary <- summary(model)
  
  # Add continent information to the model summary
  model_summary$continent <- cont
  
  # Append the model summary to the list
  model_summaries[[cont]] <- model_summary

}
```

